{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPH23ahQmxRp3MkCQfPYp7t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ravikumar-intelsat/AI-Langchain/blob/main/chunksofrepo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ],
      "metadata": {
        "id": "ivMbfoERJGmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-google-vertexai\n",
        "!pip install google-cloud-aiplatform langchain\n",
        "!pip install faiss-cpu\n",
        "!pip install  langchain-community"
      ],
      "metadata": {
        "id": "z-b_qesfJO3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ku-GY5FIDl__"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tempfile\n",
        "from git import Repo\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from google.cloud import aiplatform\n",
        "from langchain_google_vertexai import VertexAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "\n",
        "def clone_repo_to_memory(repo_url):\n",
        "    \"\"\"\n",
        "    Clone a Git repository to a temporary directory.\n",
        "    \"\"\"\n",
        "    temp_dir = tempfile.mkdtemp()\n",
        "    Repo.clone_from(repo_url, temp_dir)\n",
        "    return temp_dir\n",
        "\n",
        "def get_files_recursively(folder_path, file_extensions=None):\n",
        "    \"\"\"\n",
        "    Retrieve all files from a folder recursively, filtering by extensions.\n",
        "    \"\"\"\n",
        "    file_list = []\n",
        "    for root, _, files in os.walk(folder_path):\n",
        "        for file in files:\n",
        "            if not file_extensions or file.endswith(tuple(file_extensions)):\n",
        "                file_list.append(os.path.join(root, file))\n",
        "    return file_list\n",
        "\n",
        "def split_file_content(file_path, chunk_size=500, chunk_overlap=50):\n",
        "    \"\"\"\n",
        "    Split file content using RecursiveCharacterTextSplitter.\n",
        "    \"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        content = f.read()\n",
        "\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \" \"]  # Logical splitting\n",
        "    )\n",
        "    return splitter.create_documents([content])\n",
        "\n",
        "def process_repo(repo_url, file_extensions=None):\n",
        "    \"\"\"\n",
        "    Clone a repo, split file contents recursively.\n",
        "    \"\"\"\n",
        "    repo_path = clone_repo_to_memory(repo_url)\n",
        "    files = get_files_recursively(repo_path, file_extensions)\n",
        "    all_chunks = []\n",
        "    for file in files:\n",
        "        # print(f\"Processing: {file}\")\n",
        "        chunks = split_file_content(file)\n",
        "        all_chunks.extend(chunks)\n",
        "    return all_chunks\n",
        "        # for i, chunk in enumerate(chunks):\n",
        "        #     print(f\"Chunk {i+1}:\\n{chunk}\\n{'-'*20}\")\n",
        "\n",
        "# Example Usage\n",
        "# Initialize Google Vertex AI\n",
        "aiplatform.init(os.getenv('VERTEXAI-PROJECT-ID'), os.getenv('VERTEXAI-LOCATION'))\n",
        "embedding = VertexAIEmbeddings(model_name=\"text-embedding-004\")\n",
        "\n",
        "\n",
        "repository_url = \"https://github.com/chakra-ui/chakra-ui\"\n",
        "file_extensions = [\".ipynb\", \".js\", \".ts\", \".tsx\", \".md\"]  # Modify extensions based on your repo's content\n",
        "documents = process_repo(repository_url, file_extensions)\n",
        "# Store in Vector Database (e.g., FAISS)\n",
        "vector_db = FAISS.from_documents(documents, embedding)\n",
        "query_result = vector_db.similarity_search(\"How to update to Chakra UI V3? elobrate in 1000 words\", k=1)\n",
        "print(\"Search Result:\", query_result)\n",
        "\n"
      ],
      "metadata": {
        "id": "avwd_3neD9mr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}